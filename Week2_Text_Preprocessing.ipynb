{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad182173-35f6-4305-a0e3-77a34c6026dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  AI/ML (Python + NLP) â€” Week 2: Text Preprocessing\n",
    "\n",
    "**Assignment:**  \n",
    "1. Collect a sample dataset of user skills + task descriptions.  \n",
    "2. Preprocess text: remove punctuation & lowercase, tokenize, remove stopwords, apply stemming/lemmatization.  \n",
    "3. Generate embeddings (OpenAI or Hugging Face).\n",
    "\n",
    "**Deliverables:**  \n",
    "- `Week2_Text_Preprocessing.ipynb` (this notebook)  \n",
    "- `processed_skills_single.csv`  \n",
    "- `processed_skills_single.json`\n",
    "\n",
    "**Example given in assignment:**  \n",
    "Input: *\"Looking for a React Native developer with Firebase skills\"*  \n",
    "Output: *[\"react\", \"native\", \"developer\", \"firebase\", \"skill\"]*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67c06ea-64ed-4bd5-ada7-f9d7d6994991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Looking for a React Native developer with Firebase skills\n"
     ]
    }
   ],
   "source": [
    "# Taking my first step on the data given \n",
    "dataset = [\n",
    "    {\"id\": 1, \"text\": \"Looking for a React Native developer with Firebase skills\"}\n",
    "]\n",
    "\n",
    "# Previewing dataset\n",
    "for d in dataset:\n",
    "    print(d[\"id\"], \":\", d[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde38170-5591-43c8-a482-842d5138b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Looking for a React Native developer with Firebase skills\n",
      "Cleaned : looking for a react native developer with firebase skills\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the text\n",
    "import re  # regular expressions\n",
    "\n",
    "def clean_text(s):\n",
    "    # Lowercase\n",
    "    s = s.lower()\n",
    "    # Removing punctuation and special characters but keeping letters,numbers and spaces\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    # Remove extra spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Test cleaning on dataset\n",
    "cleaned = clean_text(dataset[0][\"text\"])\n",
    "print(\"Original:\", dataset[0][\"text\"])\n",
    "print(\"Cleaned :\", cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e4a311c-0cee-4410-b6f2-3d747614bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['looking', 'for', 'a', 'react', 'native', 'developer', 'with', 'firebase', 'skills']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "def tokenize(s):\n",
    "    return s.split()  # split on spaces\n",
    "\n",
    "tokens = tokenize(cleaned)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517abcc6-77b4-427e-8d6b-298c74406507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without stopwords: ['looking', 'react', 'native', 'developer', 'firebase', 'skills']\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords, defining a small stopword set \n",
    "STOPWORDS = {\"for\", \"a\", \"the\", \"and\", \"in\", \"of\", \"with\"}\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "tokens_nostop = remove_stopwords(tokens)\n",
    "print(\"Without stopwords:\", tokens_nostop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94abfd77-1d4f-4ccd-b093-8b689fb9cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens: ['look', 'react', 'nativ', 'develop', 'firebas', 'skill']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Applying stemming to the tokens without stopwords\n",
    "stems = [ps.stem(t) for t in tokens_nostop]\n",
    "print(\"Stemmed tokens:\", stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8e7a113-bb03-4504-b66f-9a071689adc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized tokens: ['look', 'react', 'native', 'developer', 'firebase', 'skill']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "lemmas = lemmatize_tokens(tokens_nostop)\n",
    "print(\"Lemmatized tokens:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abd455c1-2f9b-45fe-9514-c4be95e251f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files: processed_skills_single.csv, processed_skills_single.json\n"
     ]
    }
   ],
   "source": [
    "import csv, json\n",
    "\n",
    "processed = {\n",
    "    \"id\": 1,\n",
    "    \"original\": \"Looking for a React Native developer with Firebase skills\",\n",
    "    \"final_tokens\": ['look', 'react', 'native', 'developer', 'firebase', 'skill']\n",
    "}\n",
    "\n",
    "# Saving to CSV\n",
    "csv_path = \"processed_skills_single.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"original\", \"final_tokens\"])\n",
    "    writer.writerow([processed[\"id\"], processed[\"original\"], json.dumps(processed[\"final_tokens\"], ensure_ascii=False)])\n",
    "\n",
    "# Saving to JSON\n",
    "json_path = \"processed_skills_single.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved files: {csv_path}, {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f900fc7-3f2d-4059-8bcd-a928db6b105c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013d058-79e1-4e64-a6e3-21e92c8cf521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
